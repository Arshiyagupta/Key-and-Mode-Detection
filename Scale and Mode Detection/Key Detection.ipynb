{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy, matplotlib.pyplot as plt, IPython.display as ipd\n",
    "import librosa, librosa.display\n",
    "import pandas as pd\n",
    "import os\n",
    "import util\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load audio into python\"\"\"\n",
    "\n",
    "i=0\n",
    "# Please enter path of \"genres\" folder in \"basepath\" Eg. C:/GTZAN/genres/ Please ensure there is a \"/\" at the end.\n",
    "basepath = 'path'\n",
    "for folder in os.listdir(basepath):\n",
    "    folder = basepath + folder\n",
    "    for file in os.listdir(folder):\n",
    "        locals()[\"F\"+str(i)], sr = librosa.load(folder + '/' + file)\n",
    "        print(i)\n",
    "        i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define Function to determine key from Chroma vector. \n",
    "Source - https://gist.github.com/bmcfee/1f66825cef2eb34c839b42dddbad49fd\"\"\"\n",
    "\n",
    "def ks_key(X):\n",
    "    '''Estimate the key from a pitch class distribution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape=(12,)\n",
    "        Pitch-class energy distribution.  Need not be normalized\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    major : np.ndarray, shape=(12,)\n",
    "    minor : np.ndarray, shape=(12,)\n",
    "    \n",
    "        For each key (C:maj, ..., B:maj) and (C:min, ..., B:min),\n",
    "        the correlation score for `X` against that key.\n",
    "    '''\n",
    "    X = scipy.stats.zscore(X)\n",
    "    \n",
    "    # Coefficients from Kumhansl and Schmuckler\n",
    "    # as reported here: http://rnhart.net/articles/key-finding/\n",
    "    major = np.asarray([6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88])\n",
    "    major = scipy.stats.zscore(major)\n",
    "    \n",
    "    minor = np.asarray([6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17])\n",
    "    minor = scipy.stats.zscore(minor)\n",
    "    \n",
    "    # Generate all rotations of major\n",
    "    major = scipy.linalg.circulant(major)\n",
    "    minor = scipy.linalg.circulant(minor)\n",
    "    \n",
    "    return major.T.dot(X), minor.T.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate Chromagram arrays and summing along axis = 1 to get Chroma vector\"\"\"\n",
    "Scales_predicted = [\"C\",\"C#\",\"D\",\"Eb\",\"E\",\"F\",\"F#\",\"G\",\"Ab\",\"A\",\"Bb\",\"B\"]\n",
    "Keys_predicted = []\n",
    "\n",
    "for i in range (0,1000):\n",
    "    print(i)\n",
    "    chromagram = librosa.feature.chroma_cqt(locals()[\"F\"+str(i)], sr=sr)\n",
    "    summed_chromagram = np.sum((chromagram), axis = 1)\n",
    "\n",
    "\n",
    "\"\"\" Calling key finding function (ks_key) to determine confidence score for every scale\"\"\"\n",
    "    k = ks_key(summed_chromagram)\n",
    "    k_pd = pd.DataFrame(k)\n",
    "    largest_coordinate = k_pd.values.max()\n",
    "    print(largest_coordinate)\n",
    "\n",
    "\"\"\"Findind max confidence score and printing its corresponding scale\"\"\"\n",
    "\n",
    "    for j in range (0,12):\n",
    "        if k[0][j] == largest_coordinate:\n",
    "           \n",
    "            Keys_predicted.append(Scales_predicted[j] + \" major\")\n",
    "        \n",
    "        elif k[1][j] == largest_coordinate:\n",
    "\n",
    "            Keys_predicted.append(Scales_predicted[j] + \" minor\")\n",
    "\n",
    "print(Keys_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Upload data of Actual Key\"\"\"\n",
    "\n",
    "Scales_ref = [\"A major\",\"Bb major\",\"B major\",\"C major\",\"C# major\",\"D major\",\"Eb major\",\"E major\",\"F major\",\"F# major\",\"G major\",\"Ab major\",\"A minor\",\"Bb minor\",\"B minor\",\"C minor\",\"C# minor\",\"D minor\",\"Eb minor\",\"E minor\",\"F minor\",\"F# minor\",\"G minor\",\"Ab minor\"]\n",
    "\n",
    "Keys_actual=[]\n",
    "\n",
    "i=0\n",
    "# Please enter path of \"keys/genres\" folder in \"basepath\" Eg. C:/GTZAN//keys/genres/ Please ensure there is a \"/\" at the end.\n",
    "basepath = 'path'\n",
    "for folder in os.listdir(basepath):\n",
    "    folder = basepath + folder\n",
    "    for file in os.listdir(folder):\n",
    "        Key_file = open(folder + '/' + file,\"r\")\n",
    "        Key_file_data = Key_file.read()\n",
    "        \n",
    "\n",
    "        if Key_file_data != \"-1\":\n",
    "            Keys_actual.append(Scales_ref[int(Key_file_data)])\n",
    "        else:\n",
    "            Keys_actual.append('Modulating')\n",
    "        i=i+1\n",
    "        Key_file.close()\n",
    "        \n",
    "print(Keys_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define function to give weighted score to prediction\n",
    "Full score given if estimated key is same as reference key.\n",
    "50% score given if estimated key is perfect fifth above reference key.\n",
    "25% score given if estimated key is relative major / minor of referenc key.\n",
    "20% score given if estimated key is parallel major / minor of referenc key\n",
    "\n",
    "Source - https://github.com/craffel/mir_eval/blob/master/mir_eval/key.py\"\"\"\n",
    "\n",
    "KEY_TO_SEMITONE = {'c': 0, 'c#': 1, 'db': 1, 'd': 2, 'd#': 3, 'eb': 3, 'e': 4,\n",
    "                   'f': 5, 'f#': 6, 'gb': 6, 'g': 7, 'g#': 8, 'ab': 8, 'a': 9,\n",
    "                   'a#': 10, 'bb': 10, 'b': 11}\n",
    "\n",
    "\n",
    "def validate_key(key):\n",
    "    \"\"\"Checks that a key is well-formatted, e.g. in the form ``'C# major'``.\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : str\n",
    "        Key to verify\n",
    "    \"\"\"\n",
    "    if len(key.split()) != 2:\n",
    "        raise ValueError(\"'{}' is not in the form '(key) (mode)'\".format(key))\n",
    "    key, mode = key.split()\n",
    "    if key.lower() not in KEY_TO_SEMITONE:\n",
    "        raise ValueError(\n",
    "            \"Key {} is invalid; should be e.g. D or C# or Eb\".format(key))\n",
    "    if mode not in ['major', 'minor']:\n",
    "        raise ValueError(\n",
    "            \"Mode '{}' is invalid; must be 'major' or 'minor'\".format(mode))\n",
    "\n",
    "\n",
    "def validate(reference_key, estimated_key):\n",
    "    \"\"\"Checks that the input annotations to a metric are valid key strings and\n",
    "    throws helpful errors if not.\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference_key : str\n",
    "        Reference key string.\n",
    "    estimated_key : str\n",
    "        Estimated key string.\n",
    "    \"\"\"\n",
    "    for key in [reference_key, estimated_key]:\n",
    "        validate_key(key)\n",
    "\n",
    "\n",
    "def split_key_string(key):\n",
    "    \"\"\"Splits a key string (of the form, e.g. ``'C# major'``), into a tuple of\n",
    "    ``(key, mode)`` where ``key`` is is an integer representing the semitone\n",
    "    distance from C.\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : str\n",
    "        String representing a key.\n",
    "    Returns\n",
    "    -------\n",
    "    key : int\n",
    "        Number of semitones above C.\n",
    "    mode : str\n",
    "        String representing the mode.\n",
    "    \"\"\"\n",
    "    key, mode = key.split()\n",
    "    return KEY_TO_SEMITONE[key.lower()], mode\n",
    "\n",
    "\n",
    "def weighted_score(reference_key, estimated_key):\n",
    "    \"\"\"Computes a heuristic score which is weighted according to the\n",
    "    relationship of the reference and estimated key, as follows:\n",
    "    +------------------------------------------------------+-------+\n",
    "    | Relationship                                         | Score |\n",
    "    +------------------------------------------------------+-------+\n",
    "    | Same key                                             | 1.0   |\n",
    "    +------------------------------------------------------+-------+\n",
    "    | Estimated key is a perfect fifth above reference key | 0.5   |\n",
    "    +------------------------------------------------------+-------+\n",
    "    | Relative major/minor                                 | 0.3   |\n",
    "    +------------------------------------------------------+-------+\n",
    "    | Parallel major/minor                                 | 0.2   |\n",
    "    +------------------------------------------------------+-------+\n",
    "    | Other                                                | 0.0   |\n",
    "    +------------------------------------------------------+-------+\n",
    "    Examples\n",
    "    --------\n",
    "    >>> ref_key = mir_eval.io.load_key('ref.txt')\n",
    "    >>> est_key = mir_eval.io.load_key('est.txt')\n",
    "    >>> score = mir_eval.key.weighted_score(ref_key, est_key)\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference_key : str\n",
    "        Reference key string.\n",
    "    estimated_key : str\n",
    "        Estimated key string.\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        Score representing how closely related the keys are.\n",
    "    \"\"\"\n",
    "    validate(reference_key, estimated_key)\n",
    "    reference_key, reference_mode = split_key_string(reference_key)\n",
    "    estimated_key, estimated_mode = split_key_string(estimated_key)\n",
    "    # If keys are the same, return 1.\n",
    "    if reference_key == estimated_key and reference_mode == estimated_mode:\n",
    "        return 1.\n",
    "    # If keys are the same mode and a perfect fifth (differ by 7 semitones)\n",
    "    if (estimated_mode == reference_mode and\n",
    "            (estimated_key - reference_key) % 12 == 7):\n",
    "        return 0.5\n",
    "    # Estimated key is relative minor of reference key (9 semitones)\n",
    "    if (estimated_mode != reference_mode == 'major' and\n",
    "            (estimated_key - reference_key) % 12 == 9):\n",
    "        return 0.3\n",
    "    # Estimated key is relative major of reference key (3 semitones)\n",
    "    if (estimated_mode != reference_mode == 'minor' and\n",
    "            (estimated_key - reference_key) % 12 == 3):\n",
    "        return 0.3\n",
    "    # If keys are in different modes and parallel (same key name)\n",
    "    if estimated_mode != reference_mode and reference_key == estimated_key:\n",
    "        return 0.2\n",
    "    # Otherwise return 0\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def evaluate(reference_key, estimated_key, **kwargs):\n",
    "    \"\"\"Compute all metrics for the given reference and estimated annotations.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> ref_key = mir_eval.io.load_key('reference.txt')\n",
    "    >>> est_key = mir_eval.io.load_key('estimated.txt')\n",
    "    >>> scores = mir_eval.key.evaluate(ref_key, est_key)\n",
    "    Parameters\n",
    "    ----------\n",
    "    ref_key : str\n",
    "        Reference key string.\n",
    "    est_key : str\n",
    "        Estimated key string.\n",
    "    kwargs\n",
    "        Additional keyword arguments which will be passed to the\n",
    "        appropriate metric or preprocessing functions.\n",
    "    Returns\n",
    "    -------\n",
    "    scores : dict\n",
    "        Dictionary of scores, where the key is the metric name (str) and\n",
    "        the value is the (float) score achieved.\n",
    "    \"\"\"\n",
    "    # Compute all metrics\n",
    "    scores = {}\n",
    "\n",
    "    scores['Weighted Score'] = util.filter_kwargs(\n",
    "            weighted_score, reference_key, estimated_key)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Score calculation for the given model\"\"\"\n",
    "Rating = []\n",
    "for i in range (0,1000):\n",
    "    if Keys_actual[i] == 'Modulating':\n",
    "        pass\n",
    "    else:\n",
    "        r = evaluate (Keys_actual[int(i)],Keys_predicted[int(i)])\n",
    "        Rating.append(r)\n",
    "    \n",
    "\n",
    "SumRating = 0\n",
    "i = 0\n",
    "for rating in Rating:\n",
    "    Rating_ = int(rating['Weighted Score'])\n",
    "    SumRating = SumRating + Rating_\n",
    "    i = i+1\n",
    "Accuracy = str((SumRating/i)*100)\n",
    "print (\"Accuracy of above model is \" + Accuracy + \" %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
